{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1>Accelerating Applications with CUDA C/C++ (Pre-lab)</h1></div>\n",
    "<div align=\"right\">Test Jupyter and CUDA</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Accelerated Systems\n",
    "\n",
    "*Accelerated systems*, also referred to as *heterogeneous systems*, are those composed of both CPUs and GPUs. Accelerated systems run CPU programs which in turn, launch functions that will benefit from the massive parallelism provided by GPUs. This lab environment is an accelerated system which includes an NVIDIA GPU. Information about this GPU can be queried with the `nvidia-smi` (*Systems Management Interface*) command line command. Issue the `nvidia-smi` command now, by `CTRL` + `ENTER` on the code execution cell below. You will find these cells throughout this lab any time you need to execute code. The output from running the command will be printed just below the code execution cell after the code runs. After running the code execution block immediately below, take care to find and note the name of the GPU in the output.\n",
    "\n",
    "\n",
    "\n",
    "#### NOTE:\n",
    "    * Under Linux  Nvidia-SMI command is typically located in /usr/bin\n",
    "    * Under Windows-10 it is stored by default in the following location:\n",
    "    C:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm*\\nvidia-smi.exe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise: Test a Hello GPU Kernel\n",
    "\n",
    "In the line BELOW change sm_61 to the correct value depending upon your GPU\n",
    "\n",
    "\n",
    "Maxwell cards (CUDA 6 until CUDA 11)\n",
    "\n",
    "    SM50 or SM_50, compute_50 –\n",
    "    Tesla/Quadro M series.\n",
    "    Deprecated from CUDA 11, will be dropped in future versions.\n",
    "    SM52 or SM_52, compute_52 –\n",
    "    Quadro M6000 , GeForce 900, GTX-970, GTX-980, GTX Titan X.\n",
    "    SM53 or SM_53, compute_53 –\n",
    "    Tegra (Jetson) TX1 / Tegra X1, Drive CX, Drive PX, Jetson Nano.\n",
    "\n",
    "Pascal (CUDA 8 and later)\n",
    "\n",
    "    SM60 or SM_60, compute_60 –\n",
    "    Quadro GP100, Tesla P100, DGX-1 (Generic Pascal)\n",
    "    SM61 or SM_61, compute_61–\n",
    "    GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4, Discrete GPU on the NVIDIA Drive PX2\n",
    "    SM62 or SM_62, compute_62 – \n",
    "    Integrated GPU on the NVIDIA Drive PX2, Tegra (Jetson) TX2 \n",
    "    \n",
    " Volta (CUDA 9 and later)\n",
    "\n",
    "    SM70 or SM_70, compute_70 –\n",
    "    DGX-1 with Volta, Tesla V100, GTX 1180 (GV104), Titan V, Quadro GV100\n",
    "    SM72 or SM_72, compute_72 –\n",
    "    Jetson AGX Xavier, Drive AGX Pegasus, Xavier NX    \n",
    "\n",
    "Turing (CUDA 10 and later)\n",
    "\n",
    "    SM75 or SM_75, compute_75 –\n",
    "    GTX/RTX Turing – GTX 1660 Ti, RTX 2060, RTX 2070, RTX 2080, Titan RTX, Quadro RTX 4000, Quadro RTX 5000, Quadro RTX 6000, Quadro RTX 8000, Quadro T1000/T2000, Tesla T4 \n",
    "    \n",
    "Ampere (CUDA 11 and later)\n",
    "\n",
    "    SM80 or SM_80, compute_80 –\n",
    "    Tesla A100 (GA100), NVIDIA DGX-A100, RTX Ampere – RTX 3080\n",
    "    \n",
    "    SM86 or SM_86, compute 86\n",
    "    Tesla GA10x cards, RTX Ampere – RTX 3080, GA102 – RTX 3090, RTX A2000, A3000, RTX A4000, A5000, A6000, NVIDIA A40, GA106 – RTX 3060, GA104 – RTX 3070, GA107 – RTX 3050, RTX A10, RTX A16, RTX A40, A2 Tensor Core GPU\n",
    "    \n",
    "    SM87 or SM_87, compute_87\n",
    "    Jetson AGX Orin and Drive AGX Orin only\n",
    "    \n",
    "Ada Lovelace  (CUDA 11.8 and later)\n",
    "\n",
    "    SM89 or SM_89, compute_89 –\n",
    "    NVIDIA GeForce RTX 4090, RTX 4080, RTX 6000, Tesla L40\n",
    "\n",
    "Hopper (CUDA 12 and later)\n",
    "\n",
    "    SM90 or SM_90, compute_90 –\n",
    "    NVIDIA H100 (GH100)\n",
    "    \n",
    "    SM90a or SM_90a, compute_90a – (for PTX ISA version 8.0) – adds acceleration for features like wgmma and setmaxnreg. This is required for NVIDIA CUTLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_61 -o hello-gpu 01-hello-gpu-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the NVIDIA profiler\n",
    "\n",
    "The traditional tool is/was nvprof. On some newer CUDA platforms this tool is no longer supplied-- it is being phased out. Instead use nsys. \n",
    "\n",
    " --stats=true\n",
    " \n",
    " nsys profile -o foo --stats=true\n",
    " \n",
    " change nvprof as needed below\n",
    " \n",
    " !nsys profile -o foo --stats=true ./hello-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvprof ./hello-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting Up the NVIDIA Visual Profiler or Nsight Compute\n",
    "\n",
    "1) Open a terminal session and type \"nvvp\" to start the NVIDIA profiler. It is a graphical application (using Ecclipse) so it needs to run on your graphical desktop rather than in a browser (well there is a way but we won't bother).\n",
    "\n",
    "Choose workspace.\n",
    "\n",
    "2) Start the Nsight Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
